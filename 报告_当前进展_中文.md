# ROSMAP Cellpose-SAM 微调项目阶段报告（中文）

## 一、项目目标与总体设计
本项目目标是基于 ROSMAP 人脑 IF 图像（DAPI + 单 marker）完成以下工作：
- 用 Cellpose-SAM 做细胞体分割，并验证微调收益。
- 在有限标注预算下做 label efficiency 评估。
- 通过后处理和下游分析，证明分割质量提升对科学结论稳定性有价值。

本阶段已重点完成：
- E0 数据整理
- E1 基线与微调主实验
- E2 标注预算曲线（完整 25 次运行）
- E3 gating ablation

E5（全量 14,749 tiles 下游分析）目前仍待实现（代码仍是 stub，且配置目录无全量 tiles）。

---

## 二、已完成的数据处理流程（E0）

## 1. 数据格式统一
已将训练/验证/测试数据整理为 Cellpose 训练格式：
- 图像：3 通道（DAPI, marker, 0）
- mask：实例 mask（0 背景，1..N 细胞实例 id）

目录：
- `data/cellpose/train_cells`
- `data/cellpose/val_cells`
- `data/cellpose/test_cells`

## 2. 切分情况
当前切分结果：
- train: 72 张
- val: 24 张
- test: 24 张

备注：代码中 donor split 文件为 `data/splits/donor_splits.yaml`。

---

## 三、我具体做过的代码工作

## 1. 将关键模块从 stub 改为可运行
新增工具模块：
- `src/fintune/utils/dataset.py`
- `src/fintune/utils/metrics.py`

已实现或重写：
- `src/fintune/data_prep/prepare_cellpose_data.py`
- `src/fintune/inference/cpsam_baseline.py`
- `src/fintune/training/finetune_generic.py`
- `src/fintune/training/finetune_gfap.py`
- `src/fintune/training/budget_curve.py`
- `src/fintune/evaluation/gating_ablation.py`

脚本参数同步更新：
- `scripts/run_cpsam_baseline.py`
- `scripts/train_finetune_generic.py`
- `scripts/train_finetune_gfap.py`
- `scripts/run_budget_curve.py`
- `scripts/run_gating_ablation.py`

## 2. 训练流程可靠性改造
针对中断与覆盖问题做了恢复机制：
- budget curve 增加 `--skip-existing`，可只补缺失运行。
- 输出增加时间戳文件（避免覆盖历史）：
  - `records_YYYYMMDD_HHMMSS.tsv`
  - `summary_YYYYMMDD_HHMMSS.json`

---

## 四、E1 主实验结果（baseline / fine-tune）

结果表：`data/reports/e1_metrics_with_hybrid.tsv`

## 1. Overall（test）
- baseline (`cpsam`): AP50=0.105450, P=0.097836, R=0.784515, F1=0.173976
- generic fine-tune (`generic_ft`): AP50=0.235867, P=0.500888, R=0.526119, F1=0.513194
- GFAP-only fine-tune (`gfap_ft`): AP50=0.149321, P=0.236364, R=0.618470, F1=0.342017
- hybrid recipe: AP50=0.258040, P=0.463933, R=0.569963, F1=0.511511

解读：
- baseline 召回高但 precision 很低（FP 很多），F1 很差。
- generic fine-tune 的 precision/F1 提升显著，是目前最稳主模型。
- hybrid 的 AP50 最好（通过按 marker 选模型修复某些类失效）。

## 2. 关键分 marker 现象
- GFAP：generic 明显提升。
- NeuN：generic 明显提升。
- IBA1：generic precision 很高但 recall 很低。
- OLIG2：generic 失效（AP/Recall/F1 近 0），baseline 反而能工作。

---

## 五、E2 标注预算曲线（已完成完整 25 组）

配置：
- budgets = {2, 5, 10, 20, 30}
- repeats = 5
- 总运行数 = 25（已全部完成）

主要文件：
- `data/logs/budget_curve/records.tsv`
- `data/logs/budget_curve/summary.json`
- `data/reports/e2_budget_all_runs.tsv`
- `data/reports/e2_budget_summary_final.tsv`
- `data/reports/e2_budget_summary_final.json`

## 1. Overall 均值结果（val）
- budget 2: AP50=0.090595, P=0.107884, R=0.525753, F1=0.179022
- budget 5: AP50=0.093579, P=0.118561, R=0.501672, F1=0.191751
- budget 10: AP50=0.093425, P=0.118861, R=0.507191, F1=0.192515
- budget 20: AP50=0.093518, P=0.117420, R=0.507525, F1=0.190612
- budget 30: AP50=0.093036, P=0.116938, R=0.507860, F1=0.190022

解读：
- 从 budget 2 到 5 有明显提升。
- 5 之后收益趋于平台（在当前 quick setting 下）。
- 该结果支持“少量标注可获得大部分收益”的结论方向。

---

## 六、E3 后处理消融（gating ablation）

输出：
- `data/reports/e3_gating_generic_cpsam_test.tsv`

当前有效配置：
- `intensity_thresh = 0.0~1.0`（等价全强度保留）+ `min_area=50`
- 本质是“面积过滤”为主。

overall（test）：
- raw: AP50=0.235867, P=0.500888, R=0.526119, F1=0.513194
- gated: AP50=0.236231, P=0.502226, R=0.526119, F1=0.513895
- 改变量：AP +0.000364, P +0.001338, R +0.000000, F1 +0.000701

解读：
- 后处理确实稍微减少 FP，提高 precision/F1。
- 但提升幅度较小，说明当前 recipe 已较稳，后处理只是微调收益。

---

## 七、OLIG2 失效的当前排查结论

## 1. 现象确认
在 `generic_cpsam` 的 test 预测中：
- OLIG2 指标几乎全 0。
- 4 张 OLIG2 test 图里，3 张预测为 0 实例；整体平均仅 0.25 个预测实例/图。
- 而真值约 47 个实例/图（共 188 个）。

## 2. 数据规模与形态统计
OLIG2 样本统计：
- train: 17 图, 810 实例, 47.6 实例/图
- val: 9 图, 489 实例, 54.3 实例/图
- test: 4 图, 188 实例, 47.0 实例/图

实例面积（test）中位数约 138 像素，属于中小目标密集场景。

## 3. 初步推断
- 不是“没有 OLIG2 训练样本”，而是多 marker 合训时，OLIG2 分布被其他 marker 主导特征淹没。
- 同时推理阈值可能对 OLIG2 过于保守，导致几乎全部被抑制为背景。

（我已启动阈值扫描脚本做进一步验证，但该扫描尚未输出最终结论表。）

---

## 八、E1 的数据增强情况（你关心的问题）
结论：E1.2 / E1.3 **有默认数据增强**。

证据：
- 训练调用 `cellpose.train.train_seg(...)`。
- Cellpose 源码中 `train_seg` 默认 `scale_range=0.5`：
  - `/ix/jbwang/liangyou/miniconda3/envs/finetune/lib/python3.10/site-packages/cellpose/train.py:367`
- 训练与验证批次都调用 `random_rotate_and_resize(...)`：
  - 训练：`.../train.py:457-460`
  - 验证：`.../train.py:507-509`

说明：
- baseline E1.1 是纯推理，不涉及训练增强。

---

## 九、当前 sbatch 运行状态（你要求改为集群作业）

已提交作业：
- baseline: `1592743`
- generic train+eval: `1592744`
- gfap train+eval: `1592745`
- gating（依赖 generic）：`1592746`

当前状态（最近一次查询）：
- `1592743` 已完成。
- `1592744` 因 time limit 被取消（5小时不够）。
- `1592745` 仍在跑（接近 5 小时上限，存在超时风险）。
- `1592746` 处于 `DependencyNeverSatisfied`（因为 generic 失败）。

相关日志：
- `data/logs/slurm/e1_baseline_test-1592743.out`
- `data/logs/slurm/e1_generic_train_eval-1592744.err`
- `data/logs/slurm/e1_gfap_train_eval-1592745.err`

---

## 十、当前效果总评

## 1. 已经达成的结果
- 已形成可运行的 E0-E3 主流程。
- 微调相对 baseline 的提升明确且可复现（尤其 F1/precision）。
- Label efficiency 全部 25 组已跑完并汇总。
- 后处理消融已跑完并有定量结论。

## 2. 当前主要技术风险
- 多 marker 通用模型中 OLIG2 失效严重。
- sbatch 训练时长不足，导致长训练作业超时。

## 3. 建议下一步
- 将 E1 sbatch 时长提升到 >=12h，同时小幅降低 epochs（例如 80 -> 60）以保证成功收敛。
- 单独做 OLIG2-specific 微调或两阶段 recipe（先通用后 marker-specific 校正）。
- 完成 E5（full inference + donor-level correlation + bootstrap stability），补齐论文主卖点。

